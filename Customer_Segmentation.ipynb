import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
data = pd.read_excel("ONLINE_RETAIL.xlsx")

# Initial EDA
print(data.head())
print(data.info())
print("Shape:", data.shape)
print("Missing values:\n", data.isnull().sum())

# Missing value plot
missing_data = pd.DataFrame((data.isnull().sum())*100/data.shape[0]).reset_index()
missing_data.columns = ['Column', 'Missing_Percentage']
plt.figure(figsize=(16,5))
sns.pointplot(x='Column', y='Missing_Percentage',data=missing_data, color='orange')
plt.xticks(rotation=90, fontsize=7)
plt.ylabel('Percentage')
plt.xlabel('Column')
plt.title('Missing value percentage')
plt.tight_layout()
plt.show()

# Describe data
print(data.describe())

# --- Data Cleaning ---
data_clean = data[data['Customer ID'].notna()]
data_clean = data_clean[data_clean['Quantity'] > 0]

# Remove non-product stock codes
non_product_code = ['POST', 'ADJUST', 'BANK CHARGES', 'DISPATCH']
data_clean = data_clean[~data_clean['StockCode'].isin(non_product_code)]

# Drop duplicates
data_clean = data_clean.drop_duplicates()
print("Cleaned data shape:", data_clean.shape)
print(data_clean.isnull().sum())

# Total Spend feature
data_clean['TotalSpend'] = data_clean['Quantity'] * data_clean['Price']

# --- RFM Calculation ---
snapshot_date = data_clean['InvoiceDate'].max() + pd.DateOffset(days=1)
rfm = data_clean.groupby("Customer ID").agg({
    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,
    'Invoice': 'nunique',
    'TotalSpend': 'sum'
}).rename(columns={
    'InvoiceDate': 'Recency',
    'Invoice': 'Frequency',
    'TotalSpend': 'Monetary'
})

# Add unique product count
product_diversity = data_clean.groupby('Customer ID')['StockCode'].nunique().reset_index()
product_diversity.columns = ['Customer ID', 'UniqueProduct']
rfm = rfm.merge(product_diversity, on='Customer ID', how='left')

# Add country info
country_data = data_clean[['Customer ID', 'Country']].drop_duplicates()
rfm = rfm.merge(country_data, on='Customer ID', how='left')

# Cap outliers
for col in ['Monetary', 'Frequency', 'Recency']:
    rfm[col] = np.where(rfm[col] > rfm[col].quantile(0.99), rfm[col].quantile(0.99), rfm[col])

# --- Scaling ---
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X = rfm[['Recency', 'Frequency', 'Monetary', 'UniqueProduct']]
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

# --- PCA ---
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
df_pca['Customer ID'] = rfm['Customer ID']

print("PCA Explained Variance Ratio:", pca.explained_variance_ratio_)

# --- KMeans ---
from sklearn.cluster import KMeans
wcss = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

plt.figure(figsize=(10,6))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.xlabel('Number of clusters (K)')
plt.ylabel('WCSS')
plt.title('Elbow Method')
plt.show()

# Final KMeans model
kmeans = KMeans(n_clusters=5, random_state=5)
rfm['Cluster_Kmeans'] = kmeans.fit_predict(X_scaled)

# --- DBSCAN ---
from sklearn.cluster import DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
rfm['Cluster_DBSCAN'] = dbscan.fit_predict(X_scaled)
print("DBSCAN Clusters:", np.unique(rfm['Cluster_DBSCAN']))

import plotly.express as px
fig = px.scatter_3d(rfm, x='Recency', y='Frequency', z='Monetary',
                   color='Cluster_DBSCAN', title='DBSCAN Clusters')
fig.show()

# --- GMM ---
from sklearn.mixture import GaussianMixture
gmm = GaussianMixture(n_components=5, random_state=42)
rfm['Cluster_GMM'] = gmm.fit_predict(X_scaled)

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=rfm['Cluster_GMM'], cmap='viridis')
plt.title('GMM Clusters (PCA)')
plt.show()

# --- Silhouette Scores ---
from sklearn.metrics import silhouette_score

kmeans_score = silhouette_score(X_scaled, rfm['Cluster_Kmeans'])
print(f"K-Means Silhouette Score: {kmeans_score:.2f}")

dbscan_cluster = rfm[rfm['Cluster_DBSCAN'] != -1]
if isinstance(X_scaled, pd.DataFrame):
    dbscan_score = silhouette_score(X_scaled.loc[dbscan_cluster.index], dbscan_cluster['Cluster_DBSCAN'])
else:
    dbscan_score = silhouette_score(X_scaled[dbscan_cluster.index], dbscan_cluster['Cluster_DBSCAN'])
print(f"DBSCAN Silhouette Score: {dbscan_score:.2f}")

gmm_score = silhouette_score(X_scaled, rfm['Cluster_GMM'])
print(f"GMM Silhouette Score: {gmm_score:.2f}")

# --- Cluster Profiling ---
cluster_profile = rfm.groupby('Cluster_Kmeans').agg({
    'Recency': 'mean',
    'Frequency': 'mean',
    'Monetary': 'mean',
    'UniqueProduct': 'mean',
    'Country': lambda x: x.mode()[0]
}).round(2).reset_index()

# Add segment names
cluster_names = {
    0: "Regular Shoppers",
    1: "At Risk Churners",
    2: "High-Spending New Customers",
    3: "Budget Shoppers",
    4: "High Value Royalty"
}
cluster_profile['Segment'] = cluster_profile['Cluster_Kmeans'].map(cluster_names)
print(cluster_profile)

# 3D Plot of KMeans clusters
fig = px.scatter_3d(
    rfm, x='Recency', y='Frequency', z='Monetary',
    color='Cluster_Kmeans', hover_name='Customer ID',
    title='Customer Segments (K-Means)'
)
fig.show()

# Heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(cluster_profile.set_index('Segment')[['Recency', 'Frequency', 'Monetary']],
            annot=True, cmap='YlGnBu', fmt=".1f")
plt.title('Cluster Behavior Heatmap')
plt.show()

# --- Save results and model ---
rfm.to_csv('rfm_clusters.csv', index=False)

import joblib
joblib.dump(kmeans, 'kmeans_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(pca, 'pca.pkl')
